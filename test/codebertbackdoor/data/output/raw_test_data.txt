1<CODESPLIT>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L46-L108<CODESPLIT>train<CODESPLIT>Trains a k - nearest neighbors classifier for face recognition .<CODESPLIT>def train ( train_dir , model_save_path = None , n_neighbors = None , knn_algo = 'ball_tree' , verbose = False ) : X = [ ] y = [ ] # Loop through each person in the training set for class_dir in os . listdir ( train_dir ) : if not os . path . isdir ( os . path . join ( train_dir , class_dir ) ) : continue # Loop through each training image for the current person for img_path in image_files_in_folder ( os . path . join ( train_dir , class_dir ) ) : image = face_recognition . load_image_file ( img_path ) face_bounding_boxes = face_recognition . face_locations ( image ) if len ( face_bounding_boxes ) != 1 : # If there are no people (or too many people) in a training image, skip the image. if verbose : print ( "Image {} not suitable for training: {}" . format ( img_path , "Didn't find a face" if len ( face_bounding_boxes ) < 1 else "Found more than one face" ) ) else : # Add face encoding for current image to the training set X . append ( face_recognition . face_encodings ( image , known_face_locations = face_bounding_boxes ) [ 0 ] ) y . append ( class_dir ) # Determine how many neighbors to use for weighting in the KNN classifier if n_neighbors is None : n_neighbors = int ( round ( math . sqrt ( len ( X ) ) ) ) if verbose : print ( "Chose n_neighbors automatically:" , n_neighbors ) # Create and train the KNN classifier knn_clf = neighbors . KNeighborsClassifier ( n_neighbors = n_neighbors , algorithm = knn_algo , weights = 'distance' ) knn_clf . fit ( X , y ) # Save the trained KNN classifier if model_save_path is not None : with open ( model_save_path , 'wb' ) as f : pickle . dump ( knn_clf , f ) return knn_clf
1<CODESPLIT>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L111-L150<CODESPLIT>predict<CODESPLIT>Recognizes faces in given image using a trained KNN classifier<CODESPLIT>def predict ( X_img_path , knn_clf = None , model_path = None , distance_threshold = 0.6 ) : if not os . path . isfile ( X_img_path ) or os . path . splitext ( X_img_path ) [ 1 ] [ 1 : ] not in ALLOWED_EXTENSIONS : raise Exception ( "Invalid image path: {}" . format ( X_img_path ) ) if knn_clf is None and model_path is None : raise Exception ( "Must supply knn classifier either thourgh knn_clf or model_path" ) # Load a trained KNN model (if one was passed in) if knn_clf is None : with open ( model_path , 'rb' ) as f : knn_clf = pickle . load ( f ) # Load image file and find face locations X_img = face_recognition . load_image_file ( X_img_path ) X_face_locations = face_recognition . face_locations ( X_img ) # If no faces are found in the image, return an empty result. if len ( X_face_locations ) == 0 : return [ ] # Find encodings for faces in the test iamge faces_encodings = face_recognition . face_encodings ( X_img , known_face_locations = X_face_locations ) # Use the KNN model to find the best matches for the test face closest_distances = knn_clf . kneighbors ( faces_encodings , n_neighbors = 1 ) are_matches = [ closest_distances [ 0 ] [ i ] [ 0 ] <= distance_threshold for i in range ( len ( X_face_locations ) ) ] # Predict classes and remove classifications that aren't within the threshold return [ ( pred , loc ) if rec else ( "unknown" , loc ) for pred , loc , rec in zip ( knn_clf . predict ( faces_encodings ) , X_face_locations , are_matches ) ]
1<CODESPLIT>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L153-L181<CODESPLIT>show_prediction_labels_on_image<CODESPLIT>Shows the face recognition results visually .<CODESPLIT>def show_prediction_labels_on_image ( img_path , predictions ) : pil_image = Image . open ( img_path ) . convert ( "RGB" ) draw = ImageDraw . Draw ( pil_image ) for name , ( top , right , bottom , left ) in predictions : # Draw a box around the face using the Pillow module draw . rectangle ( ( ( left , top ) , ( right , bottom ) ) , outline = ( 0 , 0 , 255 ) ) # There's a bug in Pillow where it blows up with non-UTF-8 text # when using the default bitmap font name = name . encode ( "UTF-8" ) # Draw a label with a name below the face text_width , text_height = draw . textsize ( name ) draw . rectangle ( ( ( left , bottom - text_height - 10 ) , ( right , bottom ) ) , fill = ( 0 , 0 , 255 ) , outline = ( 0 , 0 , 255 ) ) draw . text ( ( left + 6 , bottom - text_height - 5 ) , name , fill = ( 255 , 255 , 255 , 255 ) ) # Remove the drawing library from memory as per the Pillow docs del draw # Display the resulting image pil_image . show ( )
1<CODESPLIT>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L32-L39<CODESPLIT>_rect_to_css<CODESPLIT>Convert a dlib rect object to a plain tuple in ( top right bottom left ) order<CODESPLIT>def _rect_to_css ( rect ) : return rect . top ( ) , rect . right ( ) , rect . bottom ( ) , rect . left ( )
1<CODESPLIT>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L52-L60<CODESPLIT>_trim_css_to_bounds<CODESPLIT>Make sure a tuple in ( top right bottom left ) order is within the bounds of the image .<CODESPLIT>def _trim_css_to_bounds ( css , image_shape ) : return max ( css [ 0 ] , 0 ) , min ( css [ 1 ] , image_shape [ 1 ] ) , min ( css [ 2 ] , image_shape [ 0 ] ) , max ( css [ 3 ] , 0 )
1<CODESPLIT>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L63-L75<CODESPLIT>face_distance<CODESPLIT>Given a list of face encodings compare them to a known face encoding and get a euclidean distance for each comparison face . The distance tells you how similar the faces are .<CODESPLIT>def face_distance ( face_encodings , face_to_compare ) : if len ( face_encodings ) == 0 : return np . empty ( ( 0 ) ) return np . linalg . norm ( face_encodings - face_to_compare , axis = 1 )
1<CODESPLIT>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L78-L89<CODESPLIT>load_image_file<CODESPLIT>Loads an image file ( . jpg . png etc ) into a numpy array<CODESPLIT>def load_image_file ( file , mode = 'RGB' ) : im = PIL . Image . open ( file ) if mode : im = im . convert ( mode ) return np . array ( im )
1<CODESPLIT>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L92-L105<CODESPLIT>_raw_face_locations<CODESPLIT>Returns an array of bounding boxes of human faces in a image<CODESPLIT>def _raw_face_locations ( img , number_of_times_to_upsample = 1 , model = "hog" ) : if model == "cnn" : return cnn_face_detector ( img , number_of_times_to_upsample ) else : return face_detector ( img , number_of_times_to_upsample )
1<CODESPLIT>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L108-L121<CODESPLIT>face_locations<CODESPLIT>Returns an array of bounding boxes of human faces in a image<CODESPLIT>def face_locations ( img , number_of_times_to_upsample = 1 , model = "hog" ) : if model == "cnn" : return [ _trim_css_to_bounds ( _rect_to_css ( face . rect ) , img . shape ) for face in _raw_face_locations ( img , number_of_times_to_upsample , "cnn" ) ] else : return [ _trim_css_to_bounds ( _rect_to_css ( face ) , img . shape ) for face in _raw_face_locations ( img , number_of_times_to_upsample , model ) ]
1<CODESPLIT>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L135-L151<CODESPLIT>batch_face_locations<CODESPLIT>Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector If you are using a GPU this can give you much faster results since the GPU can process batches of images at once . If you aren t using a GPU you don t need this function .<CODESPLIT>def batch_face_locations ( images , number_of_times_to_upsample = 1 , batch_size = 128 ) : def convert_cnn_detections_to_css ( detections ) : return [ _trim_css_to_bounds ( _rect_to_css ( face . rect ) , images [ 0 ] . shape ) for face in detections ] raw_detections_batched = _raw_face_locations_batched ( images , number_of_times_to_upsample , batch_size ) return list ( map ( convert_cnn_detections_to_css , raw_detections_batched ) )
1<CODESPLIT>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L168-L200<CODESPLIT>face_landmarks<CODESPLIT>Given an image returns a dict of face feature locations ( eyes nose etc ) for each face in the image<CODESPLIT>def face_landmarks ( face_image , face_locations = None , model = "large" ) : landmarks = _raw_face_landmarks ( face_image , face_locations , model ) landmarks_as_tuples = [ [ ( p . x , p . y ) for p in landmark . parts ( ) ] for landmark in landmarks ] # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png if model == 'large' : return [ { "chin" : points [ 0 : 17 ] , "left_eyebrow" : points [ 17 : 22 ] , "right_eyebrow" : points [ 22 : 27 ] , "nose_bridge" : points [ 27 : 31 ] , "nose_tip" : points [ 31 : 36 ] , "left_eye" : points [ 36 : 42 ] , "right_eye" : points [ 42 : 48 ] , "top_lip" : points [ 48 : 55 ] + [ points [ 64 ] ] + [ points [ 63 ] ] + [ points [ 62 ] ] + [ points [ 61 ] ] + [ points [ 60 ] ] , "bottom_lip" : points [ 54 : 60 ] + [ points [ 48 ] ] + [ points [ 60 ] ] + [ points [ 67 ] ] + [ points [ 66 ] ] + [ points [ 65 ] ] + [ points [ 64 ] ] } for points in landmarks_as_tuples ] elif model == 'small' : return [ { "nose_tip" : [ points [ 4 ] ] , "left_eye" : points [ 2 : 4 ] , "right_eye" : points [ 0 : 2 ] , } for points in landmarks_as_tuples ] else : raise ValueError ( "Invalid landmarks model type. Supported models are ['small', 'large']." )
1<CODESPLIT>https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L203-L213<CODESPLIT>face_encodings<CODESPLIT>Given an image return the 128 - dimension face encoding for each face in the image .<CODESPLIT>def face_encodings ( face_image , known_face_locations = None , num_jitters = 1 ) : raw_landmarks = _raw_face_landmarks ( face_image , known_face_locations , model = "small" ) return [ np . array ( face_encoder . compute_face_descriptor ( face_image , raw_landmark_set , num_jitters ) ) for raw_landmark_set in raw_landmarks ]
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L758-L820<CODESPLIT>_parse_datatype_string<CODESPLIT>Parses the given data type string to a : class : DataType . The data type string format equals to : class : DataType . simpleString except that top level struct type can omit the struct< > and atomic types use typeName () as their format e . g . use byte instead of tinyint for : class : ByteType . We can also use int as a short name for : class : IntegerType . Since Spark 2 . 3 this also supports a schema in a DDL - formatted string and case - insensitive strings .<CODESPLIT>def _parse_datatype_string ( s ) : sc = SparkContext . _active_spark_context def from_ddl_schema ( type_str ) : return _parse_datatype_json_string ( sc . _jvm . org . apache . spark . sql . types . StructType . fromDDL ( type_str ) . json ( ) ) def from_ddl_datatype ( type_str ) : return _parse_datatype_json_string ( sc . _jvm . org . apache . spark . sql . api . python . PythonSQLUtils . parseDataType ( type_str ) . json ( ) ) try : # DDL format, "fieldname datatype, fieldname datatype". return from_ddl_schema ( s ) except Exception as e : try : # For backwards compatibility, "integer", "struct<fieldname: datatype>" and etc. return from_ddl_datatype ( s ) except : try : # For backwards compatibility, "fieldname: datatype, fieldname: datatype" case. return from_ddl_datatype ( "struct<%s>" % s . strip ( ) ) except : raise e
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L944-L955<CODESPLIT>_int_size_to_type<CODESPLIT>Return the Catalyst datatype from the size of integers .<CODESPLIT>def _int_size_to_type ( size ) : if size <= 8 : return ByteType if size <= 16 : return ShortType if size <= 32 : return IntegerType if size <= 64 : return LongType
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1003-L1038<CODESPLIT>_infer_type<CODESPLIT>Infer the DataType from obj<CODESPLIT>def _infer_type ( obj ) : if obj is None : return NullType ( ) if hasattr ( obj , '__UDT__' ) : return obj . __UDT__ dataType = _type_mappings . get ( type ( obj ) ) if dataType is DecimalType : # the precision and scale of `obj` may be different from row to row. return DecimalType ( 38 , 18 ) elif dataType is not None : return dataType ( ) if isinstance ( obj , dict ) : for key , value in obj . items ( ) : if key is not None and value is not None : return MapType ( _infer_type ( key ) , _infer_type ( value ) , True ) return MapType ( NullType ( ) , NullType ( ) , True ) elif isinstance ( obj , list ) : for v in obj : if v is not None : return ArrayType ( _infer_type ( obj [ 0 ] ) , True ) return ArrayType ( NullType ( ) , True ) elif isinstance ( obj , array ) : if obj . typecode in _array_type_mappings : return ArrayType ( _array_type_mappings [ obj . typecode ] ( ) , False ) else : raise TypeError ( "not supported type: array(%s)" % obj . typecode ) else : try : return _infer_schema ( obj ) except TypeError : raise TypeError ( "not supported type: %s" % type ( obj ) )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1041-L1065<CODESPLIT>_infer_schema<CODESPLIT>Infer the schema from dict / namedtuple / object<CODESPLIT>def _infer_schema ( row , names = None ) : if isinstance ( row , dict ) : items = sorted ( row . items ( ) ) elif isinstance ( row , ( tuple , list ) ) : if hasattr ( row , "__fields__" ) : # Row items = zip ( row . __fields__ , tuple ( row ) ) elif hasattr ( row , "_fields" ) : # namedtuple items = zip ( row . _fields , tuple ( row ) ) else : if names is None : names = [ '_%d' % i for i in range ( 1 , len ( row ) + 1 ) ] elif len ( names ) < len ( row ) : names . extend ( '_%d' % i for i in range ( len ( names ) + 1 , len ( row ) + 1 ) ) items = zip ( names , row ) elif hasattr ( row , "__dict__" ) : # object items = sorted ( row . __dict__ . items ( ) ) else : raise TypeError ( "Can not infer schema for type: %s" % type ( row ) ) fields = [ StructField ( k , _infer_type ( v ) , True ) for k , v in items ] return StructType ( fields )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1068-L1077<CODESPLIT>_has_nulltype<CODESPLIT>Return whether there is NullType in dt or not<CODESPLIT>def _has_nulltype ( dt ) : if isinstance ( dt , StructType ) : return any ( _has_nulltype ( f . dataType ) for f in dt . fields ) elif isinstance ( dt , ArrayType ) : return _has_nulltype ( ( dt . elementType ) ) elif isinstance ( dt , MapType ) : return _has_nulltype ( dt . keyType ) or _has_nulltype ( dt . valueType ) else : return isinstance ( dt , NullType )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1133-L1180<CODESPLIT>_create_converter<CODESPLIT>Create a converter to drop the names of fields in obj<CODESPLIT>def _create_converter ( dataType ) : if not _need_converter ( dataType ) : return lambda x : x if isinstance ( dataType , ArrayType ) : conv = _create_converter ( dataType . elementType ) return lambda row : [ conv ( v ) for v in row ] elif isinstance ( dataType , MapType ) : kconv = _create_converter ( dataType . keyType ) vconv = _create_converter ( dataType . valueType ) return lambda row : dict ( ( kconv ( k ) , vconv ( v ) ) for k , v in row . items ( ) ) elif isinstance ( dataType , NullType ) : return lambda x : None elif not isinstance ( dataType , StructType ) : return lambda x : x # dataType must be StructType names = [ f . name for f in dataType . fields ] converters = [ _create_converter ( f . dataType ) for f in dataType . fields ] convert_fields = any ( _need_converter ( f . dataType ) for f in dataType . fields ) def convert_struct ( obj ) : if obj is None : return if isinstance ( obj , ( tuple , list ) ) : if convert_fields : return tuple ( conv ( v ) for v , conv in zip ( obj , converters ) ) else : return tuple ( obj ) if isinstance ( obj , dict ) : d = obj elif hasattr ( obj , "__dict__" ) : # object d = obj . __dict__ else : raise TypeError ( "Unexpected obj type: %s" % type ( obj ) ) if convert_fields : return tuple ( [ conv ( d . get ( name ) ) for name , conv in zip ( names , converters ) ] ) else : return tuple ( [ d . get ( name ) for name in names ] ) return convert_struct
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1202-L1391<CODESPLIT>_make_type_verifier<CODESPLIT>Make a verifier that checks the type of obj against dataType and raises a TypeError if they do not match .<CODESPLIT>def _make_type_verifier ( dataType , nullable = True , name = None ) : if name is None : new_msg = lambda msg : msg new_name = lambda n : "field %s" % n else : new_msg = lambda msg : "%s: %s" % ( name , msg ) new_name = lambda n : "field %s in %s" % ( n , name ) def verify_nullability ( obj ) : if obj is None : if nullable : return True else : raise ValueError ( new_msg ( "This field is not nullable, but got None" ) ) else : return False _type = type ( dataType ) def assert_acceptable_types ( obj ) : assert _type in _acceptable_types , new_msg ( "unknown datatype: %s for object %r" % ( dataType , obj ) ) def verify_acceptable_types ( obj ) : # subclass of them can not be fromInternal in JVM if type ( obj ) not in _acceptable_types [ _type ] : raise TypeError ( new_msg ( "%s can not accept object %r in type %s" % ( dataType , obj , type ( obj ) ) ) ) if isinstance ( dataType , StringType ) : # StringType can work with any types verify_value = lambda _ : _ elif isinstance ( dataType , UserDefinedType ) : verifier = _make_type_verifier ( dataType . sqlType ( ) , name = name ) def verify_udf ( obj ) : if not ( hasattr ( obj , '__UDT__' ) and obj . __UDT__ == dataType ) : raise ValueError ( new_msg ( "%r is not an instance of type %r" % ( obj , dataType ) ) ) verifier ( dataType . toInternal ( obj ) ) verify_value = verify_udf elif isinstance ( dataType , ByteType ) : def verify_byte ( obj ) : assert_acceptable_types ( obj ) verify_acceptable_types ( obj ) if obj < - 128 or obj > 127 : raise ValueError ( new_msg ( "object of ByteType out of range, got: %s" % obj ) ) verify_value = verify_byte elif isinstance ( dataType , ShortType ) : def verify_short ( obj ) : assert_acceptable_types ( obj ) verify_acceptable_types ( obj ) if obj < - 32768 or obj > 32767 : raise ValueError ( new_msg ( "object of ShortType out of range, got: %s" % obj ) ) verify_value = verify_short elif isinstance ( dataType , IntegerType ) : def verify_integer ( obj ) : assert_acceptable_types ( obj ) verify_acceptable_types ( obj ) if obj < - 2147483648 or obj > 2147483647 : raise ValueError ( new_msg ( "object of IntegerType out of range, got: %s" % obj ) ) verify_value = verify_integer elif isinstance ( dataType , ArrayType ) : element_verifier = _make_type_verifier ( dataType . elementType , dataType . containsNull , name = "element in array %s" % name ) def verify_array ( obj ) : assert_acceptable_types ( obj ) verify_acceptable_types ( obj ) for i in obj : element_verifier ( i ) verify_value = verify_array elif isinstance ( dataType , MapType ) : key_verifier = _make_type_verifier ( dataType . keyType , False , name = "key of map %s" % name ) value_verifier = _make_type_verifier ( dataType . valueType , dataType . valueContainsNull , name = "value of map %s" % name ) def verify_map ( obj ) : assert_acceptable_types ( obj ) verify_acceptable_types ( obj ) for k , v in obj . items ( ) : key_verifier ( k ) value_verifier ( v ) verify_value = verify_map elif isinstance ( dataType , StructType ) : verifiers = [ ] for f in dataType . fields : verifier = _make_type_verifier ( f . dataType , f . nullable , name = new_name ( f . name ) ) verifiers . append ( ( f . name , verifier ) ) def verify_struct ( obj ) : assert_acceptable_types ( obj ) if isinstance ( obj , dict ) : for f , verifier in verifiers : verifier ( obj . get ( f ) ) elif isinstance ( obj , Row ) and getattr ( obj , "__from_dict__" , False ) : # the order in obj could be different than dataType.fields for f , verifier in verifiers : verifier ( obj [ f ] ) elif isinstance ( obj , ( tuple , list ) ) : if len ( obj ) != len ( verifiers ) : raise ValueError ( new_msg ( "Length of object (%d) does not match with " "length of fields (%d)" % ( len ( obj ) , len ( verifiers ) ) ) ) for v , ( _ , verifier ) in zip ( obj , verifiers ) : verifier ( v ) elif hasattr ( obj , "__dict__" ) : d = obj . __dict__ for f , verifier in verifiers : verifier ( d . get ( f ) ) else : raise TypeError ( new_msg ( "StructType can not accept object %r in type %s" % ( obj , type ( obj ) ) ) ) verify_value = verify_struct else : def verify_default ( obj ) : assert_acceptable_types ( obj ) verify_acceptable_types ( obj ) verify_value = verify_default def verify ( obj ) : if not verify_nullability ( obj ) : verify_value ( obj ) return verify
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1581-L1622<CODESPLIT>to_arrow_type<CODESPLIT>Convert Spark data type to pyarrow type<CODESPLIT>def to_arrow_type ( dt ) : import pyarrow as pa if type ( dt ) == BooleanType : arrow_type = pa . bool_ ( ) elif type ( dt ) == ByteType : arrow_type = pa . int8 ( ) elif type ( dt ) == ShortType : arrow_type = pa . int16 ( ) elif type ( dt ) == IntegerType : arrow_type = pa . int32 ( ) elif type ( dt ) == LongType : arrow_type = pa . int64 ( ) elif type ( dt ) == FloatType : arrow_type = pa . float32 ( ) elif type ( dt ) == DoubleType : arrow_type = pa . float64 ( ) elif type ( dt ) == DecimalType : arrow_type = pa . decimal128 ( dt . precision , dt . scale ) elif type ( dt ) == StringType : arrow_type = pa . string ( ) elif type ( dt ) == BinaryType : arrow_type = pa . binary ( ) elif type ( dt ) == DateType : arrow_type = pa . date32 ( ) elif type ( dt ) == TimestampType : # Timestamps should be in UTC, JVM Arrow timestamps require a timezone to be read arrow_type = pa . timestamp ( 'us' , tz = 'UTC' ) elif type ( dt ) == ArrayType : if type ( dt . elementType ) in [ StructType , TimestampType ] : raise TypeError ( "Unsupported type in conversion to Arrow: " + str ( dt ) ) arrow_type = pa . list_ ( to_arrow_type ( dt . elementType ) ) elif type ( dt ) == StructType : if any ( type ( field . dataType ) == StructType for field in dt ) : raise TypeError ( "Nested StructType not supported in conversion to Arrow" ) fields = [ pa . field ( field . name , to_arrow_type ( field . dataType ) , nullable = field . nullable ) for field in dt ] arrow_type = pa . struct ( fields ) else : raise TypeError ( "Unsupported type in conversion to Arrow: " + str ( dt ) ) return arrow_type
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1625-L1631<CODESPLIT>to_arrow_schema<CODESPLIT>Convert a schema from Spark to Arrow<CODESPLIT>def to_arrow_schema ( schema ) : import pyarrow as pa fields = [ pa . field ( field . name , to_arrow_type ( field . dataType ) , nullable = field . nullable ) for field in schema ] return pa . schema ( fields )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1634-L1674<CODESPLIT>from_arrow_type<CODESPLIT>Convert pyarrow type to Spark data type .<CODESPLIT>def from_arrow_type ( at ) : import pyarrow . types as types if types . is_boolean ( at ) : spark_type = BooleanType ( ) elif types . is_int8 ( at ) : spark_type = ByteType ( ) elif types . is_int16 ( at ) : spark_type = ShortType ( ) elif types . is_int32 ( at ) : spark_type = IntegerType ( ) elif types . is_int64 ( at ) : spark_type = LongType ( ) elif types . is_float32 ( at ) : spark_type = FloatType ( ) elif types . is_float64 ( at ) : spark_type = DoubleType ( ) elif types . is_decimal ( at ) : spark_type = DecimalType ( precision = at . precision , scale = at . scale ) elif types . is_string ( at ) : spark_type = StringType ( ) elif types . is_binary ( at ) : spark_type = BinaryType ( ) elif types . is_date32 ( at ) : spark_type = DateType ( ) elif types . is_timestamp ( at ) : spark_type = TimestampType ( ) elif types . is_list ( at ) : if types . is_timestamp ( at . value_type ) : raise TypeError ( "Unsupported type in conversion from Arrow: " + str ( at ) ) spark_type = ArrayType ( from_arrow_type ( at . value_type ) ) elif types . is_struct ( at ) : if any ( types . is_struct ( field . type ) for field in at ) : raise TypeError ( "Nested StructType not supported in conversion from Arrow: " + str ( at ) ) return StructType ( [ StructField ( field . name , from_arrow_type ( field . type ) , nullable = field . nullable ) for field in at ] ) else : raise TypeError ( "Unsupported type in conversion from Arrow: " + str ( at ) ) return spark_type
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1677-L1682<CODESPLIT>from_arrow_schema<CODESPLIT>Convert schema from Arrow to Spark .<CODESPLIT>def from_arrow_schema ( arrow_schema ) : return StructType ( [ StructField ( field . name , from_arrow_type ( field . type ) , nullable = field . nullable ) for field in arrow_schema ] )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1700-L1720<CODESPLIT>_check_series_localize_timestamps<CODESPLIT>Convert timezone aware timestamps to timezone - naive in the specified timezone or local timezone .<CODESPLIT>def _check_series_localize_timestamps ( s , timezone ) : from pyspark . sql . utils import require_minimum_pandas_version require_minimum_pandas_version ( ) from pandas . api . types import is_datetime64tz_dtype tz = timezone or _get_local_timezone ( ) # TODO: handle nested timestamps, such as ArrayType(TimestampType())? if is_datetime64tz_dtype ( s . dtype ) : return s . dt . tz_convert ( tz ) . dt . tz_localize ( None ) else : return s
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1723-L1736<CODESPLIT>_check_dataframe_localize_timestamps<CODESPLIT>Convert timezone aware timestamps to timezone - naive in the specified timezone or local timezone<CODESPLIT>def _check_dataframe_localize_timestamps ( pdf , timezone ) : from pyspark . sql . utils import require_minimum_pandas_version require_minimum_pandas_version ( ) for column , series in pdf . iteritems ( ) : pdf [ column ] = _check_series_localize_timestamps ( series , timezone ) return pdf
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1739-L1789<CODESPLIT>_check_series_convert_timestamps_internal<CODESPLIT>Convert a tz - naive timestamp in the specified timezone or local timezone to UTC normalized for Spark internal storage<CODESPLIT>def _check_series_convert_timestamps_internal ( s , timezone ) : from pyspark . sql . utils import require_minimum_pandas_version require_minimum_pandas_version ( ) from pandas . api . types import is_datetime64_dtype , is_datetime64tz_dtype # TODO: handle nested timestamps, such as ArrayType(TimestampType())? if is_datetime64_dtype ( s . dtype ) : # When tz_localize a tz-naive timestamp, the result is ambiguous if the tz-naive # timestamp is during the hour when the clock is adjusted backward during due to # daylight saving time (dst). # E.g., for America/New_York, the clock is adjusted backward on 2015-11-01 2:00 to # 2015-11-01 1:00 from dst-time to standard time, and therefore, when tz_localize # a tz-naive timestamp 2015-11-01 1:30 with America/New_York timezone, it can be either # dst time (2015-01-01 1:30-0400) or standard time (2015-11-01 1:30-0500). # # Here we explicit choose to use standard time. This matches the default behavior of # pytz. # # Here are some code to help understand this behavior: # >>> import datetime # >>> import pandas as pd # >>> import pytz # >>> # >>> t = datetime.datetime(2015, 11, 1, 1, 30) # >>> ts = pd.Series([t]) # >>> tz = pytz.timezone('America/New_York') # >>> # >>> ts.dt.tz_localize(tz, ambiguous=True) # 0   2015-11-01 01:30:00-04:00 # dtype: datetime64[ns, America/New_York] # >>> # >>> ts.dt.tz_localize(tz, ambiguous=False) # 0   2015-11-01 01:30:00-05:00 # dtype: datetime64[ns, America/New_York] # >>> # >>> str(tz.localize(t)) # '2015-11-01 01:30:00-05:00' tz = timezone or _get_local_timezone ( ) return s . dt . tz_localize ( tz , ambiguous = False ) . dt . tz_convert ( 'UTC' ) elif is_datetime64tz_dtype ( s . dtype ) : return s . dt . tz_convert ( 'UTC' ) else : return s
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1792-L1817<CODESPLIT>_check_series_convert_timestamps_localize<CODESPLIT>Convert timestamp to timezone - naive in the specified timezone or local timezone<CODESPLIT>def _check_series_convert_timestamps_localize ( s , from_timezone , to_timezone ) : from pyspark . sql . utils import require_minimum_pandas_version require_minimum_pandas_version ( ) import pandas as pd from pandas . api . types import is_datetime64tz_dtype , is_datetime64_dtype from_tz = from_timezone or _get_local_timezone ( ) to_tz = to_timezone or _get_local_timezone ( ) # TODO: handle nested timestamps, such as ArrayType(TimestampType())? if is_datetime64tz_dtype ( s . dtype ) : return s . dt . tz_convert ( to_tz ) . dt . tz_localize ( None ) elif is_datetime64_dtype ( s . dtype ) and from_tz != to_tz : # `s.dt.tz_localize('tzlocal()')` doesn't work properly when including NaT. return s . apply ( lambda ts : ts . tz_localize ( from_tz , ambiguous = False ) . tz_convert ( to_tz ) . tz_localize ( None ) if ts is not pd . NaT else pd . NaT ) else : return s
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L491-L537<CODESPLIT>StructType.add<CODESPLIT>Construct a StructType by adding new elements to it to define the schema . The method accepts either :<CODESPLIT>def add ( self , field , data_type = None , nullable = True , metadata = None ) : if isinstance ( field , StructField ) : self . fields . append ( field ) self . names . append ( field . name ) else : if isinstance ( field , str ) and data_type is None : raise ValueError ( "Must specify DataType if passing name of struct_field to create." ) if isinstance ( data_type , str ) : data_type_f = _parse_datatype_json_value ( data_type ) else : data_type_f = data_type self . fields . append ( StructField ( field , data_type_f , nullable , metadata ) ) self . names . append ( field ) # Precalculated list of fields that need conversion with fromInternal/toInternal functions self . _needConversion = [ f . needConversion ( ) for f in self ] self . _needSerializeAnyField = any ( self . _needConversion ) return self
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L675-L681<CODESPLIT>UserDefinedType._cachedSqlType<CODESPLIT>Cache the sqlType () into class because it s heavy used in toInternal .<CODESPLIT>def _cachedSqlType ( cls ) : if not hasattr ( cls , "_cached_sql_type" ) : cls . _cached_sql_type = cls . sqlType ( ) return cls . _cached_sql_type
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1463-L1492<CODESPLIT>Row.asDict<CODESPLIT>Return as an dict<CODESPLIT>def asDict ( self , recursive = False ) : if not hasattr ( self , "__fields__" ) : raise TypeError ( "Cannot convert a Row class into dict" ) if recursive : def conv ( obj ) : if isinstance ( obj , Row ) : return obj . asDict ( True ) elif isinstance ( obj , list ) : return [ conv ( o ) for o in obj ] elif isinstance ( obj , dict ) : return dict ( ( k , conv ( v ) ) for k , v in obj . items ( ) ) else : return obj return dict ( zip ( self . __fields__ , ( conv ( o ) for o in self ) ) ) else : return dict ( zip ( self . __fields__ , self ) )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/ml/regression.py#L198-L208<CODESPLIT>LinearRegressionModel.summary<CODESPLIT>Gets summary ( e . g . residuals mse r - squared ) of model on training set . An exception is thrown if trainingSummary is None .<CODESPLIT>def summary ( self ) : if self . hasSummary : return LinearRegressionTrainingSummary ( super ( LinearRegressionModel , self ) . summary ) else : raise RuntimeError ( "No training summary available for this %s" % self . __class__ . __name__ )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/ml/regression.py#L211-L222<CODESPLIT>LinearRegressionModel.evaluate<CODESPLIT>Evaluates the model on a test dataset .<CODESPLIT>def evaluate ( self , dataset ) : if not isinstance ( dataset , DataFrame ) : raise ValueError ( "dataset must be a DataFrame but got %s." % type ( dataset ) ) java_lr_summary = self . _call_java ( "evaluate" , dataset ) return LinearRegressionSummary ( java_lr_summary )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/ml/regression.py#L1679-L1690<CODESPLIT>GeneralizedLinearRegressionModel.summary<CODESPLIT>Gets summary ( e . g . residuals deviance pValues ) of model on training set . An exception is thrown if trainingSummary is None .<CODESPLIT>def summary ( self ) : if self . hasSummary : return GeneralizedLinearRegressionTrainingSummary ( super ( GeneralizedLinearRegressionModel , self ) . summary ) else : raise RuntimeError ( "No training summary available for this %s" % self . __class__ . __name__ )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/ml/regression.py#L1693-L1704<CODESPLIT>GeneralizedLinearRegressionModel.evaluate<CODESPLIT>Evaluates the model on a test dataset .<CODESPLIT>def evaluate ( self , dataset ) : if not isinstance ( dataset , DataFrame ) : raise ValueError ( "dataset must be a DataFrame but got %s." % type ( dataset ) ) java_glr_summary = self . _call_java ( "evaluate" , dataset ) return GeneralizedLinearRegressionSummary ( java_glr_summary )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L71-L79<CODESPLIT>_get_local_dirs<CODESPLIT>Get all the directories<CODESPLIT>def _get_local_dirs ( sub ) : path = os . environ . get ( "SPARK_LOCAL_DIRS" , "/tmp" ) dirs = path . split ( "," ) if len ( dirs ) > 1 : # different order in different processes and instances rnd = random . Random ( os . getpid ( ) + id ( dirs ) ) random . shuffle ( dirs , rnd . random ) return [ os . path . join ( d , "python" , str ( os . getpid ( ) ) , sub ) for d in dirs ]
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L219-L221<CODESPLIT>ExternalMerger._get_spill_dir<CODESPLIT>Choose one directory for spill by number n<CODESPLIT>def _get_spill_dir ( self , n ) : return os . path . join ( self . localdirs [ n % len ( self . localdirs ) ] , str ( n ) )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L231-L253<CODESPLIT>ExternalMerger.mergeValues<CODESPLIT>Combine the items by creator and combiner<CODESPLIT>def mergeValues ( self , iterator ) : # speedup attribute lookup creator , comb = self . agg . createCombiner , self . agg . mergeValue c , data , pdata , hfun , batch = 0 , self . data , self . pdata , self . _partition , self . batch limit = self . memory_limit for k , v in iterator : d = pdata [ hfun ( k ) ] if pdata else data d [ k ] = comb ( d [ k ] , v ) if k in d else creator ( v ) c += 1 if c >= batch : if get_used_memory ( ) >= limit : self . _spill ( ) limit = self . _next_limit ( ) batch /= 2 c = 0 else : batch *= 1.5 if get_used_memory ( ) >= limit : self . _spill ( )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L265-L289<CODESPLIT>ExternalMerger.mergeCombiners<CODESPLIT>Merge ( K V ) pair by mergeCombiner<CODESPLIT>def mergeCombiners ( self , iterator , limit = None ) : if limit is None : limit = self . memory_limit # speedup attribute lookup comb , hfun , objsize = self . agg . mergeCombiners , self . _partition , self . _object_size c , data , pdata , batch = 0 , self . data , self . pdata , self . batch for k , v in iterator : d = pdata [ hfun ( k ) ] if pdata else data d [ k ] = comb ( d [ k ] , v ) if k in d else v if not limit : continue c += objsize ( v ) if c > batch : if get_used_memory ( ) > limit : self . _spill ( ) limit = self . _next_limit ( ) batch /= 2 c = 0 else : batch *= 1.5 if limit and get_used_memory ( ) >= limit : self . _spill ( )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L291-L337<CODESPLIT>ExternalMerger._spill<CODESPLIT>dump already partitioned data into disks .<CODESPLIT>def _spill ( self ) : global MemoryBytesSpilled , DiskBytesSpilled path = self . _get_spill_dir ( self . spills ) if not os . path . exists ( path ) : os . makedirs ( path ) used_memory = get_used_memory ( ) if not self . pdata : # The data has not been partitioned, it will iterator the # dataset once, write them into different files, has no # additional memory. It only called when the memory goes # above limit at the first time. # open all the files for writing streams = [ open ( os . path . join ( path , str ( i ) ) , 'wb' ) for i in range ( self . partitions ) ] for k , v in self . data . items ( ) : h = self . _partition ( k ) # put one item in batch, make it compatible with load_stream # it will increase the memory if dump them in batch self . serializer . dump_stream ( [ ( k , v ) ] , streams [ h ] ) for s in streams : DiskBytesSpilled += s . tell ( ) s . close ( ) self . data . clear ( ) self . pdata . extend ( [ { } for i in range ( self . partitions ) ] ) else : for i in range ( self . partitions ) : p = os . path . join ( path , str ( i ) ) with open ( p , "wb" ) as f : # dump items in batch self . serializer . dump_stream ( iter ( self . pdata [ i ] . items ( ) ) , f ) self . pdata [ i ] . clear ( ) DiskBytesSpilled += os . path . getsize ( p ) self . spills += 1 gc . collect ( ) # release the memory as much as possible MemoryBytesSpilled += max ( used_memory - get_used_memory ( ) , 0 ) << 20
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L339-L343<CODESPLIT>ExternalMerger.items<CODESPLIT>Return all merged items as iterator<CODESPLIT>def items ( self ) : if not self . pdata and not self . spills : return iter ( self . data . items ( ) ) return self . _external_items ( )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L345-L364<CODESPLIT>ExternalMerger._external_items<CODESPLIT>Return all partitioned items as iterator<CODESPLIT>def _external_items ( self ) : assert not self . data if any ( self . pdata ) : self . _spill ( ) # disable partitioning and spilling when merge combiners from disk self . pdata = [ ] try : for i in range ( self . partitions ) : for v in self . _merged_items ( i ) : yield v self . data . clear ( ) # remove the merged partition for j in range ( self . spills ) : path = self . _get_spill_dir ( j ) os . remove ( os . path . join ( path , str ( i ) ) ) finally : self . _cleanup ( )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L386-L409<CODESPLIT>ExternalMerger._recursive_merged_items<CODESPLIT>merge the partitioned items and return the as iterator<CODESPLIT>def _recursive_merged_items ( self , index ) : subdirs = [ os . path . join ( d , "parts" , str ( index ) ) for d in self . localdirs ] m = ExternalMerger ( self . agg , self . memory_limit , self . serializer , subdirs , self . scale * self . partitions , self . partitions , self . batch ) m . pdata = [ { } for _ in range ( self . partitions ) ] limit = self . _next_limit ( ) for j in range ( self . spills ) : path = self . _get_spill_dir ( j ) p = os . path . join ( path , str ( index ) ) with open ( p , 'rb' ) as f : m . mergeCombiners ( self . serializer . load_stream ( f ) , 0 ) if get_used_memory ( ) > limit : m . _spill ( ) limit = self . _next_limit ( ) return m . _external_items ( )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L440-L445<CODESPLIT>ExternalSorter._get_path<CODESPLIT>Choose one directory for spill by number n<CODESPLIT>def _get_path ( self , n ) : d = self . local_dirs [ n % len ( self . local_dirs ) ] if not os . path . exists ( d ) : os . makedirs ( d ) return os . path . join ( d , str ( n ) )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L455-L501<CODESPLIT>ExternalSorter.sorted<CODESPLIT>Sort the elements in iterator do external sort when the memory goes above the limit .<CODESPLIT>def sorted ( self , iterator , key = None , reverse = False ) : global MemoryBytesSpilled , DiskBytesSpilled batch , limit = 100 , self . _next_limit ( ) chunks , current_chunk = [ ] , [ ] iterator = iter ( iterator ) while True : # pick elements in batch chunk = list ( itertools . islice ( iterator , batch ) ) current_chunk . extend ( chunk ) if len ( chunk ) < batch : break used_memory = get_used_memory ( ) if used_memory > limit : # sort them inplace will save memory current_chunk . sort ( key = key , reverse = reverse ) path = self . _get_path ( len ( chunks ) ) with open ( path , 'wb' ) as f : self . serializer . dump_stream ( current_chunk , f ) def load ( f ) : for v in self . serializer . load_stream ( f ) : yield v # close the file explicit once we consume all the items # to avoid ResourceWarning in Python3 f . close ( ) chunks . append ( load ( open ( path , 'rb' ) ) ) current_chunk = [ ] MemoryBytesSpilled += max ( used_memory - get_used_memory ( ) , 0 ) << 20 DiskBytesSpilled += os . path . getsize ( path ) os . unlink ( path ) # data will be deleted after close elif not chunks : batch = min ( int ( batch * 1.5 ) , 10000 ) current_chunk . sort ( key = key , reverse = reverse ) if not chunks : return current_chunk if current_chunk : chunks . append ( iter ( current_chunk ) ) return heapq . merge ( chunks , key = key , reverse = reverse )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L590-L602<CODESPLIT>ExternalList._spill<CODESPLIT>dump the values into disk<CODESPLIT>def _spill ( self ) : global MemoryBytesSpilled , DiskBytesSpilled if self . _file is None : self . _open_file ( ) used_memory = get_used_memory ( ) pos = self . _file . tell ( ) self . _ser . dump_stream ( self . values , self . _file ) self . values = [ ] gc . collect ( ) DiskBytesSpilled += self . _file . tell ( ) - pos MemoryBytesSpilled += max ( used_memory - get_used_memory ( ) , 0 ) << 20
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L709-L766<CODESPLIT>ExternalGroupBy._spill<CODESPLIT>dump already partitioned data into disks .<CODESPLIT>def _spill ( self ) : global MemoryBytesSpilled , DiskBytesSpilled path = self . _get_spill_dir ( self . spills ) if not os . path . exists ( path ) : os . makedirs ( path ) used_memory = get_used_memory ( ) if not self . pdata : # The data has not been partitioned, it will iterator the # data once, write them into different files, has no # additional memory. It only called when the memory goes # above limit at the first time. # open all the files for writing streams = [ open ( os . path . join ( path , str ( i ) ) , 'wb' ) for i in range ( self . partitions ) ] # If the number of keys is small, then the overhead of sort is small # sort them before dumping into disks self . _sorted = len ( self . data ) < self . SORT_KEY_LIMIT if self . _sorted : self . serializer = self . flattened_serializer ( ) for k in sorted ( self . data . keys ( ) ) : h = self . _partition ( k ) self . serializer . dump_stream ( [ ( k , self . data [ k ] ) ] , streams [ h ] ) else : for k , v in self . data . items ( ) : h = self . _partition ( k ) self . serializer . dump_stream ( [ ( k , v ) ] , streams [ h ] ) for s in streams : DiskBytesSpilled += s . tell ( ) s . close ( ) self . data . clear ( ) # self.pdata is cached in `mergeValues` and `mergeCombiners` self . pdata . extend ( [ { } for i in range ( self . partitions ) ] ) else : for i in range ( self . partitions ) : p = os . path . join ( path , str ( i ) ) with open ( p , "wb" ) as f : # dump items in batch if self . _sorted : # sort by key only (stable) sorted_items = sorted ( self . pdata [ i ] . items ( ) , key = operator . itemgetter ( 0 ) ) self . serializer . dump_stream ( sorted_items , f ) else : self . serializer . dump_stream ( self . pdata [ i ] . items ( ) , f ) self . pdata [ i ] . clear ( ) DiskBytesSpilled += os . path . getsize ( p ) self . spills += 1 gc . collect ( ) # release the memory as much as possible MemoryBytesSpilled += max ( used_memory - get_used_memory ( ) , 0 ) << 20
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L786-L808<CODESPLIT>ExternalGroupBy._merge_sorted_items<CODESPLIT>load a partition from disk then sort and group by key<CODESPLIT>def _merge_sorted_items ( self , index ) : def load_partition ( j ) : path = self . _get_spill_dir ( j ) p = os . path . join ( path , str ( index ) ) with open ( p , 'rb' , 65536 ) as f : for v in self . serializer . load_stream ( f ) : yield v disk_items = [ load_partition ( j ) for j in range ( self . spills ) ] if self . _sorted : # all the partitions are already sorted sorted_items = heapq . merge ( disk_items , key = operator . itemgetter ( 0 ) ) else : # Flatten the combined values, so it will not consume huge # memory during merging sort. ser = self . flattened_serializer ( ) sorter = ExternalSorter ( self . memory_limit , ser ) sorted_items = sorter . sorted ( itertools . chain ( * disk_items ) , key = operator . itemgetter ( 0 ) ) return ( ( k , vs ) for k , vs in GroupByKey ( sorted_items ) )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/daemon.py#L43-L81<CODESPLIT>worker<CODESPLIT>Called by a worker process after the fork () .<CODESPLIT>def worker ( sock , authenticated ) : signal . signal ( SIGHUP , SIG_DFL ) signal . signal ( SIGCHLD , SIG_DFL ) signal . signal ( SIGTERM , SIG_DFL ) # restore the handler for SIGINT, # it's useful for debugging (show the stacktrace before exit) signal . signal ( SIGINT , signal . default_int_handler ) # Read the socket using fdopen instead of socket.makefile() because the latter # seems to be very slow; note that we need to dup() the file descriptor because # otherwise writes also cause a seek that makes us miss data on the read side. infile = os . fdopen ( os . dup ( sock . fileno ( ) ) , "rb" , 65536 ) outfile = os . fdopen ( os . dup ( sock . fileno ( ) ) , "wb" , 65536 ) if not authenticated : client_secret = UTF8Deserializer ( ) . loads ( infile ) if os . environ [ "PYTHON_WORKER_FACTORY_SECRET" ] == client_secret : write_with_length ( "ok" . encode ( "utf-8" ) , outfile ) outfile . flush ( ) else : write_with_length ( "err" . encode ( "utf-8" ) , outfile ) outfile . flush ( ) sock . close ( ) return 1 exit_code = 0 try : worker_main ( infile , outfile ) except SystemExit as exc : exit_code = compute_real_exit_code ( exc . code ) finally : try : outfile . flush ( ) except Exception : pass return exit_code
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L78-L106<CODESPLIT>portable_hash<CODESPLIT>This function returns consistent hash code for builtin types especially for None and tuple with None .<CODESPLIT>def portable_hash ( x ) : if sys . version_info >= ( 3 , 2 , 3 ) and 'PYTHONHASHSEED' not in os . environ : raise Exception ( "Randomness of hash of string should be disabled via PYTHONHASHSEED" ) if x is None : return 0 if isinstance ( x , tuple ) : h = 0x345678 for i in x : h ^= portable_hash ( i ) h *= 1000003 h &= sys . maxsize h ^= len ( x ) if h == - 1 : h = - 2 return int ( h ) return hash ( x )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L125-L138<CODESPLIT>_parse_memory<CODESPLIT>Parse a memory string in the format supported by Java ( e . g . 1g 200m ) and return the value in MiB<CODESPLIT>def _parse_memory ( s ) : units = { 'g' : 1024 , 'm' : 1 , 't' : 1 << 20 , 'k' : 1.0 / 1024 } if s [ - 1 ] . lower ( ) not in units : raise ValueError ( "invalid format: " + s ) return int ( float ( s [ : - 1 ] ) * units [ s [ - 1 ] . lower ( ) ] )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L150-L160<CODESPLIT>ignore_unicode_prefix<CODESPLIT>Ignore the u prefix of string in doc tests to make it works in both python 2 and 3<CODESPLIT>def ignore_unicode_prefix ( f ) : if sys . version >= '3' : # the representation of unicode string in Python 3 does not have prefix 'u', # so remove the prefix 'u' for doc tests literal_re = re . compile ( r"(\W|^)[uU](['])" , re . UNICODE ) f . __doc__ = literal_re . sub ( r'\1\2' , f . __doc__ ) return f
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L223-L229<CODESPLIT>RDD.cache<CODESPLIT>Persist this RDD with the default storage level ( C { MEMORY_ONLY } ) .<CODESPLIT>def cache ( self ) : self . is_cached = True self . persist ( StorageLevel . MEMORY_ONLY ) return self
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L231-L245<CODESPLIT>RDD.persist<CODESPLIT>Set this RDD s storage level to persist its values across operations after the first time it is computed . This can only be used to assign a new storage level if the RDD does not have a storage level set yet . If no storage level is specified defaults to ( C { MEMORY_ONLY } ) .<CODESPLIT>def persist ( self , storageLevel = StorageLevel . MEMORY_ONLY ) : self . is_cached = True javaStorageLevel = self . ctx . _getJavaStorageLevel ( storageLevel ) self . _jrdd . persist ( javaStorageLevel ) return self
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L247-L258<CODESPLIT>RDD.unpersist<CODESPLIT>Mark the RDD as non - persistent and remove all blocks for it from memory and disk .<CODESPLIT>def unpersist ( self , blocking = False ) : self . is_cached = False self . _jrdd . unpersist ( blocking ) return self
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L307-L315<CODESPLIT>RDD.getCheckpointFile<CODESPLIT>Gets the name of the file to which this RDD was checkpointed<CODESPLIT>def getCheckpointFile ( self ) : checkpointFile = self . _jrdd . rdd ( ) . getCheckpointFile ( ) if checkpointFile . isDefined ( ) : return checkpointFile . get ( )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L317-L327<CODESPLIT>RDD.map<CODESPLIT>Return a new RDD by applying a function to each element of this RDD .<CODESPLIT>def map ( self , f , preservesPartitioning = False ) : def func ( _ , iterator ) : return map ( fail_on_stopiteration ( f ) , iterator ) return self . mapPartitionsWithIndex ( func , preservesPartitioning )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L329-L342<CODESPLIT>RDD.flatMap<CODESPLIT>Return a new RDD by first applying a function to all elements of this RDD and then flattening the results .<CODESPLIT>def flatMap ( self , f , preservesPartitioning = False ) : def func ( s , iterator ) : return chain . from_iterable ( map ( fail_on_stopiteration ( f ) , iterator ) ) return self . mapPartitionsWithIndex ( func , preservesPartitioning )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L344-L355<CODESPLIT>RDD.mapPartitions<CODESPLIT>Return a new RDD by applying a function to each partition of this RDD .<CODESPLIT>def mapPartitions ( self , f , preservesPartitioning = False ) : def func ( s , iterator ) : return f ( iterator ) return self . mapPartitionsWithIndex ( func , preservesPartitioning )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L369-L383<CODESPLIT>RDD.mapPartitionsWithSplit<CODESPLIT>Deprecated : use mapPartitionsWithIndex instead .<CODESPLIT>def mapPartitionsWithSplit ( self , f , preservesPartitioning = False ) : warnings . warn ( "mapPartitionsWithSplit is deprecated; " "use mapPartitionsWithIndex instead" , DeprecationWarning , stacklevel = 2 ) return self . mapPartitionsWithIndex ( f , preservesPartitioning )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L407-L416<CODESPLIT>RDD.distinct<CODESPLIT>Return a new RDD containing the distinct elements in this RDD .<CODESPLIT>def distinct ( self , numPartitions = None ) : return self . map ( lambda x : ( x , None ) ) . reduceByKey ( lambda x , _ : x , numPartitions ) . map ( lambda x : x [ 0 ] )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L418-L436<CODESPLIT>RDD.sample<CODESPLIT>Return a sampled subset of this RDD .<CODESPLIT>def sample ( self , withReplacement , fraction , seed = None ) : assert fraction >= 0.0 , "Negative fraction value: %s" % fraction return self . mapPartitionsWithIndex ( RDDSampler ( withReplacement , fraction , seed ) . func , True )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L438-L462<CODESPLIT>RDD.randomSplit<CODESPLIT>Randomly splits this RDD with the provided weights .<CODESPLIT>def randomSplit ( self , weights , seed = None ) : s = float ( sum ( weights ) ) cweights = [ 0.0 ] for w in weights : cweights . append ( cweights [ - 1 ] + w / s ) if seed is None : seed = random . randint ( 0 , 2 ** 32 - 1 ) return [ self . mapPartitionsWithIndex ( RDDRangeSampler ( lb , ub , seed ) . func , True ) for lb , ub in zip ( cweights , cweights [ 1 : ] ) ]
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L465-L518<CODESPLIT>RDD.takeSample<CODESPLIT>Return a fixed - size sampled subset of this RDD .<CODESPLIT>def takeSample ( self , withReplacement , num , seed = None ) : numStDev = 10.0 if num < 0 : raise ValueError ( "Sample size cannot be negative." ) elif num == 0 : return [ ] initialCount = self . count ( ) if initialCount == 0 : return [ ] rand = random . Random ( seed ) if ( not withReplacement ) and num >= initialCount : # shuffle current RDD and return samples = self . collect ( ) rand . shuffle ( samples ) return samples maxSampleSize = sys . maxsize - int ( numStDev * sqrt ( sys . maxsize ) ) if num > maxSampleSize : raise ValueError ( "Sample size cannot be greater than %d." % maxSampleSize ) fraction = RDD . _computeFractionForSampleSize ( num , initialCount , withReplacement ) samples = self . sample ( withReplacement , fraction , seed ) . collect ( ) # If the first sample didn't turn out large enough, keep trying to take samples; # this shouldn't happen often because we use a big multiplier for their initial size. # See: scala/spark/RDD.scala while len ( samples ) < num : # TODO: add log warning for when more than one iteration was run seed = rand . randint ( 0 , sys . maxsize ) samples = self . sample ( withReplacement , fraction , seed ) . collect ( ) rand . shuffle ( samples ) return samples [ 0 : num ]
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L521-L551<CODESPLIT>RDD._computeFractionForSampleSize<CODESPLIT>Returns a sampling rate that guarantees a sample of size > = sampleSizeLowerBound 99 . 99% of the time .<CODESPLIT>def _computeFractionForSampleSize ( sampleSizeLowerBound , total , withReplacement ) : fraction = float ( sampleSizeLowerBound ) / total if withReplacement : numStDev = 5 if ( sampleSizeLowerBound < 12 ) : numStDev = 9 return fraction + numStDev * sqrt ( fraction / total ) else : delta = 0.00005 gamma = - log ( delta ) / total return min ( 1 , fraction + gamma + sqrt ( gamma * gamma + 2 * gamma * fraction ) )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L553-L574<CODESPLIT>RDD.union<CODESPLIT>Return the union of this RDD and another one .<CODESPLIT>def union ( self , other ) : if self . _jrdd_deserializer == other . _jrdd_deserializer : rdd = RDD ( self . _jrdd . union ( other . _jrdd ) , self . ctx , self . _jrdd_deserializer ) else : # These RDDs contain data in different serialized formats, so we # must normalize them to the default serializer. self_copy = self . _reserialize ( ) other_copy = other . _reserialize ( ) rdd = RDD ( self_copy . _jrdd . union ( other_copy . _jrdd ) , self . ctx , self . ctx . serializer ) if ( self . partitioner == other . partitioner and self . getNumPartitions ( ) == rdd . getNumPartitions ( ) ) : rdd . partitioner = self . partitioner return rdd
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L576-L591<CODESPLIT>RDD.intersection<CODESPLIT>Return the intersection of this RDD and another one . The output will not contain any duplicate elements even if the input RDDs did .<CODESPLIT>def intersection ( self , other ) : return self . map ( lambda v : ( v , None ) ) . cogroup ( other . map ( lambda v : ( v , None ) ) ) . filter ( lambda k_vs : all ( k_vs [ 1 ] ) ) . keys ( )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L612-L633<CODESPLIT>RDD.repartitionAndSortWithinPartitions<CODESPLIT>Repartition the RDD according to the given partitioner and within each resulting partition sort records by their keys .<CODESPLIT>def repartitionAndSortWithinPartitions ( self , numPartitions = None , partitionFunc = portable_hash , ascending = True , keyfunc = lambda x : x ) : if numPartitions is None : numPartitions = self . _defaultReducePartitions ( ) memory = _parse_memory ( self . ctx . _conf . get ( "spark.python.worker.memory" , "512m" ) ) serializer = self . _jrdd_deserializer def sortPartition ( iterator ) : sort = ExternalSorter ( memory * 0.9 , serializer ) . sorted return iter ( sort ( iterator , key = lambda k_v : keyfunc ( k_v [ 0 ] ) , reverse = ( not ascending ) ) ) return self . partitionBy ( numPartitions , partitionFunc ) . mapPartitions ( sortPartition , True )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L635-L689<CODESPLIT>RDD.sortByKey<CODESPLIT>Sorts this RDD which is assumed to consist of ( key value ) pairs .<CODESPLIT>def sortByKey ( self , ascending = True , numPartitions = None , keyfunc = lambda x : x ) : if numPartitions is None : numPartitions = self . _defaultReducePartitions ( ) memory = self . _memory_limit ( ) serializer = self . _jrdd_deserializer def sortPartition ( iterator ) : sort = ExternalSorter ( memory * 0.9 , serializer ) . sorted return iter ( sort ( iterator , key = lambda kv : keyfunc ( kv [ 0 ] ) , reverse = ( not ascending ) ) ) if numPartitions == 1 : if self . getNumPartitions ( ) > 1 : self = self . coalesce ( 1 ) return self . mapPartitions ( sortPartition , True ) # first compute the boundary of each part via sampling: we want to partition # the key-space into bins such that the bins have roughly the same # number of (key, value) pairs falling into them rddSize = self . count ( ) if not rddSize : return self # empty RDD maxSampleSize = numPartitions * 20.0 # constant from Spark's RangePartitioner fraction = min ( maxSampleSize / max ( rddSize , 1 ) , 1.0 ) samples = self . sample ( False , fraction , 1 ) . map ( lambda kv : kv [ 0 ] ) . collect ( ) samples = sorted ( samples , key = keyfunc ) # we have numPartitions many parts but one of the them has # an implicit boundary bounds = [ samples [ int ( len ( samples ) * ( i + 1 ) / numPartitions ) ] for i in range ( 0 , numPartitions - 1 ) ] def rangePartitioner ( k ) : p = bisect . bisect_left ( bounds , keyfunc ( k ) ) if ascending : return p else : return numPartitions - 1 - p return self . partitionBy ( numPartitions , rangePartitioner ) . mapPartitions ( sortPartition , True )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L691-L701<CODESPLIT>RDD.sortBy<CODESPLIT>Sorts this RDD by the given keyfunc<CODESPLIT>def sortBy ( self , keyfunc , ascending = True , numPartitions = None ) : return self . keyBy ( keyfunc ) . sortByKey ( ascending , numPartitions ) . values ( )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L716-L729<CODESPLIT>RDD.cartesian<CODESPLIT>Return the Cartesian product of this RDD and another one that is the RDD of all pairs of elements C { ( a b ) } where C { a } is in C { self } and C { b } is in C { other } .<CODESPLIT>def cartesian ( self , other ) : # Due to batching, we can't use the Java cartesian method. deserializer = CartesianDeserializer ( self . _jrdd_deserializer , other . _jrdd_deserializer ) return RDD ( self . _jrdd . cartesian ( other . _jrdd ) , self . ctx , deserializer )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L731-L740<CODESPLIT>RDD.groupBy<CODESPLIT>Return an RDD of grouped items .<CODESPLIT>def groupBy ( self , f , numPartitions = None , partitionFunc = portable_hash ) : return self . map ( lambda x : ( f ( x ) , x ) ) . groupByKey ( numPartitions , partitionFunc )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L743-L776<CODESPLIT>RDD.pipe<CODESPLIT>Return an RDD created by piping elements to a forked external process .<CODESPLIT>def pipe ( self , command , env = None , checkCode = False ) : if env is None : env = dict ( ) def func ( iterator ) : pipe = Popen ( shlex . split ( command ) , env = env , stdin = PIPE , stdout = PIPE ) def pipe_objs ( out ) : for obj in iterator : s = unicode ( obj ) . rstrip ( '\n' ) + '\n' out . write ( s . encode ( 'utf-8' ) ) out . close ( ) Thread ( target = pipe_objs , args = [ pipe . stdin ] ) . start ( ) def check_return_code ( ) : pipe . wait ( ) if checkCode and pipe . returncode : raise Exception ( "Pipe function `%s' exited " "with error code %d" % ( command , pipe . returncode ) ) else : for i in range ( 0 ) : yield i return ( x . rstrip ( b'\n' ) . decode ( 'utf-8' ) for x in chain ( iter ( pipe . stdout . readline , b'' ) , check_return_code ( ) ) ) return self . mapPartitions ( func )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L778-L791<CODESPLIT>RDD.foreach<CODESPLIT>Applies a function to all elements of this RDD .<CODESPLIT>def foreach ( self , f ) : f = fail_on_stopiteration ( f ) def processPartition ( iterator ) : for x in iterator : f ( x ) return iter ( [ ] ) self . mapPartitions ( processPartition ) . count ( )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L793-L808<CODESPLIT>RDD.foreachPartition<CODESPLIT>Applies a function to each partition of this RDD .<CODESPLIT>def foreachPartition ( self , f ) : def func ( it ) : r = f ( it ) try : return iter ( r ) except TypeError : return iter ( [ ] ) self . mapPartitions ( func ) . count ( )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L810-L819<CODESPLIT>RDD.collect<CODESPLIT>Return a list that contains all of the elements in this RDD .<CODESPLIT>def collect ( self ) : with SCCallSiteSync ( self . context ) as css : sock_info = self . ctx . _jvm . PythonRDD . collectAndServe ( self . _jrdd . rdd ( ) ) return list ( _load_from_socket ( sock_info , self . _jrdd_deserializer ) )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L821-L849<CODESPLIT>RDD.reduce<CODESPLIT>Reduces the elements of this RDD using the specified commutative and associative binary operator . Currently reduces partitions locally .<CODESPLIT>def reduce ( self , f ) : f = fail_on_stopiteration ( f ) def func ( iterator ) : iterator = iter ( iterator ) try : initial = next ( iterator ) except StopIteration : return yield reduce ( f , iterator , initial ) vals = self . mapPartitions ( func ) . collect ( ) if vals : return reduce ( f , vals ) raise ValueError ( "Can not reduce() empty RDD" )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L851-L886<CODESPLIT>RDD.treeReduce<CODESPLIT>Reduces the elements of this RDD in a multi - level tree pattern .<CODESPLIT>def treeReduce ( self , f , depth = 2 ) : if depth < 1 : raise ValueError ( "Depth cannot be smaller than 1 but got %d." % depth ) zeroValue = None , True # Use the second entry to indicate whether this is a dummy value. def op ( x , y ) : if x [ 1 ] : return y elif y [ 1 ] : return x else : return f ( x [ 0 ] , y [ 0 ] ) , False reduced = self . map ( lambda x : ( x , False ) ) . treeAggregate ( zeroValue , op , op , depth ) if reduced [ 1 ] : raise ValueError ( "Cannot reduce empty RDD." ) return reduced [ 0 ]
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L888-L920<CODESPLIT>RDD.fold<CODESPLIT>Aggregate the elements of each partition and then the results for all the partitions using a given associative function and a neutral zero value .<CODESPLIT>def fold ( self , zeroValue , op ) : op = fail_on_stopiteration ( op ) def func ( iterator ) : acc = zeroValue for obj in iterator : acc = op ( acc , obj ) yield acc # collecting result of mapPartitions here ensures that the copy of # zeroValue provided to each partition is unique from the one provided # to the final reduce call vals = self . mapPartitions ( func ) . collect ( ) return reduce ( op , vals , zeroValue )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L922-L955<CODESPLIT>RDD.aggregate<CODESPLIT>Aggregate the elements of each partition and then the results for all the partitions using a given combine functions and a neutral zero value .<CODESPLIT>def aggregate ( self , zeroValue , seqOp , combOp ) : seqOp = fail_on_stopiteration ( seqOp ) combOp = fail_on_stopiteration ( combOp ) def func ( iterator ) : acc = zeroValue for obj in iterator : acc = seqOp ( acc , obj ) yield acc # collecting result of mapPartitions here ensures that the copy of # zeroValue provided to each partition is unique from the one provided # to the final reduce call vals = self . mapPartitions ( func ) . collect ( ) return reduce ( combOp , vals , zeroValue )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L957-L1007<CODESPLIT>RDD.treeAggregate<CODESPLIT>Aggregates the elements of this RDD in a multi - level tree pattern .<CODESPLIT>def treeAggregate ( self , zeroValue , seqOp , combOp , depth = 2 ) : if depth < 1 : raise ValueError ( "Depth cannot be smaller than 1 but got %d." % depth ) if self . getNumPartitions ( ) == 0 : return zeroValue def aggregatePartition ( iterator ) : acc = zeroValue for obj in iterator : acc = seqOp ( acc , obj ) yield acc partiallyAggregated = self . mapPartitions ( aggregatePartition ) numPartitions = partiallyAggregated . getNumPartitions ( ) scale = max ( int ( ceil ( pow ( numPartitions , 1.0 / depth ) ) ) , 2 ) # If creating an extra level doesn't help reduce the wall-clock time, we stop the tree # aggregation. while numPartitions > scale + numPartitions / scale : numPartitions /= scale curNumPartitions = int ( numPartitions ) def mapPartition ( i , iterator ) : for obj in iterator : yield ( i % curNumPartitions , obj ) partiallyAggregated = partiallyAggregated . mapPartitionsWithIndex ( mapPartition ) . reduceByKey ( combOp , curNumPartitions ) . values ( ) return partiallyAggregated . reduce ( combOp )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1009-L1023<CODESPLIT>RDD.max<CODESPLIT>Find the maximum item in this RDD .<CODESPLIT>def max ( self , key = None ) : if key is None : return self . reduce ( max ) return self . reduce ( lambda a , b : max ( a , b , key = key ) )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1025-L1039<CODESPLIT>RDD.min<CODESPLIT>Find the minimum item in this RDD .<CODESPLIT>def min ( self , key = None ) : if key is None : return self . reduce ( min ) return self . reduce ( lambda a , b : min ( a , b , key = key ) )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1041-L1048<CODESPLIT>RDD.sum<CODESPLIT>Add up the elements in this RDD .<CODESPLIT>def sum ( self ) : return self . mapPartitions ( lambda x : [ sum ( x ) ] ) . fold ( 0 , operator . add )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1059-L1067<CODESPLIT>RDD.stats<CODESPLIT>Return a L { StatCounter } object that captures the mean variance and count of the RDD s elements in one operation .<CODESPLIT>def stats ( self ) : def redFunc ( left_counter , right_counter ) : return left_counter . mergeStats ( right_counter ) return self . mapPartitions ( lambda i : [ StatCounter ( i ) ] ) . reduce ( redFunc )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1069-L1195<CODESPLIT>RDD.histogram<CODESPLIT>Compute a histogram using the provided buckets . The buckets are all open to the right except for the last which is closed . e . g . [ 1 10 20 50 ] means the buckets are [ 1 10 ) [ 10 20 ) [ 20 50 ] which means 1< = x<10 10< = x<20 20< = x< = 50 . And on the input of 1 and 50 we would have a histogram of 1 0 1 .<CODESPLIT>def histogram ( self , buckets ) : if isinstance ( buckets , int ) : if buckets < 1 : raise ValueError ( "number of buckets must be >= 1" ) # filter out non-comparable elements def comparable ( x ) : if x is None : return False if type ( x ) is float and isnan ( x ) : return False return True filtered = self . filter ( comparable ) # faster than stats() def minmax ( a , b ) : return min ( a [ 0 ] , b [ 0 ] ) , max ( a [ 1 ] , b [ 1 ] ) try : minv , maxv = filtered . map ( lambda x : ( x , x ) ) . reduce ( minmax ) except TypeError as e : if " empty " in str ( e ) : raise ValueError ( "can not generate buckets from empty RDD" ) raise if minv == maxv or buckets == 1 : return [ minv , maxv ] , [ filtered . count ( ) ] try : inc = ( maxv - minv ) / buckets except TypeError : raise TypeError ( "Can not generate buckets with non-number in RDD" ) if isinf ( inc ) : raise ValueError ( "Can not generate buckets with infinite value" ) # keep them as integer if possible inc = int ( inc ) if inc * buckets != maxv - minv : inc = ( maxv - minv ) * 1.0 / buckets buckets = [ i * inc + minv for i in range ( buckets ) ] buckets . append ( maxv ) # fix accumulated error even = True elif isinstance ( buckets , ( list , tuple ) ) : if len ( buckets ) < 2 : raise ValueError ( "buckets should have more than one value" ) if any ( i is None or isinstance ( i , float ) and isnan ( i ) for i in buckets ) : raise ValueError ( "can not have None or NaN in buckets" ) if sorted ( buckets ) != list ( buckets ) : raise ValueError ( "buckets should be sorted" ) if len ( set ( buckets ) ) != len ( buckets ) : raise ValueError ( "buckets should not contain duplicated values" ) minv = buckets [ 0 ] maxv = buckets [ - 1 ] even = False inc = None try : steps = [ buckets [ i + 1 ] - buckets [ i ] for i in range ( len ( buckets ) - 1 ) ] except TypeError : pass # objects in buckets do not support '-' else : if max ( steps ) - min ( steps ) < 1e-10 : # handle precision errors even = True inc = ( maxv - minv ) / ( len ( buckets ) - 1 ) else : raise TypeError ( "buckets should be a list or tuple or number(int or long)" ) def histogram ( iterator ) : counters = [ 0 ] * len ( buckets ) for i in iterator : if i is None or ( type ( i ) is float and isnan ( i ) ) or i > maxv or i < minv : continue t = ( int ( ( i - minv ) / inc ) if even else bisect . bisect_right ( buckets , i ) - 1 ) counters [ t ] += 1 # add last two together last = counters . pop ( ) counters [ - 1 ] += last return [ counters ] def mergeCounters ( a , b ) : return [ i + j for i , j in zip ( a , b ) ] return buckets , self . mapPartitions ( histogram ) . reduce ( mergeCounters )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1245-L1263<CODESPLIT>RDD.countByValue<CODESPLIT>Return the count of each unique value in this RDD as a dictionary of ( value count ) pairs .<CODESPLIT>def countByValue ( self ) : def countPartition ( iterator ) : counts = defaultdict ( int ) for obj in iterator : counts [ obj ] += 1 yield counts def mergeMaps ( m1 , m2 ) : for k , v in m2 . items ( ) : m1 [ k ] += v return m1 return self . mapPartitions ( countPartition ) . reduce ( mergeMaps )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1265-L1287<CODESPLIT>RDD.top<CODESPLIT>Get the top N elements from an RDD .<CODESPLIT>def top ( self , num , key = None ) : def topIterator ( iterator ) : yield heapq . nlargest ( num , iterator , key = key ) def merge ( a , b ) : return heapq . nlargest ( num , a + b , key = key ) return self . mapPartitions ( topIterator ) . reduce ( merge )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1289-L1306<CODESPLIT>RDD.takeOrdered<CODESPLIT>Get the N elements from an RDD ordered in ascending order or as specified by the optional key function .<CODESPLIT>def takeOrdered ( self , num , key = None ) : def merge ( a , b ) : return heapq . nsmallest ( num , a + b , key ) return self . mapPartitions ( lambda it : [ heapq . nsmallest ( num , it , key ) ] ) . reduce ( merge )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1308-L1367<CODESPLIT>RDD.take<CODESPLIT>Take the first num elements of the RDD .<CODESPLIT>def take ( self , num ) : items = [ ] totalParts = self . getNumPartitions ( ) partsScanned = 0 while len ( items ) < num and partsScanned < totalParts : # The number of partitions to try in this iteration. # It is ok for this number to be greater than totalParts because # we actually cap it at totalParts in runJob. numPartsToTry = 1 if partsScanned > 0 : # If we didn't find any rows after the previous iteration, # quadruple and retry.  Otherwise, interpolate the number of # partitions we need to try, but overestimate it by 50%. # We also cap the estimation in the end. if len ( items ) == 0 : numPartsToTry = partsScanned * 4 else : # the first parameter of max is >=1 whenever partsScanned >= 2 numPartsToTry = int ( 1.5 * num * partsScanned / len ( items ) ) - partsScanned numPartsToTry = min ( max ( numPartsToTry , 1 ) , partsScanned * 4 ) left = num - len ( items ) def takeUpToNumLeft ( iterator ) : iterator = iter ( iterator ) taken = 0 while taken < left : try : yield next ( iterator ) except StopIteration : return taken += 1 p = range ( partsScanned , min ( partsScanned + numPartsToTry , totalParts ) ) res = self . context . runJob ( self , takeUpToNumLeft , p ) items += res partsScanned += numPartsToTry return items [ : num ]
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1398-L1412<CODESPLIT>RDD.saveAsNewAPIHadoopDataset<CODESPLIT>Output a Python RDD of key - value pairs ( of form C { RDD [ ( K V ) ] } ) to any Hadoop file system using the new Hadoop OutputFormat API ( mapreduce package ) . Keys / values are converted for output using either user specified converters or by default L { org . apache . spark . api . python . JavaToWritableConverter } .<CODESPLIT>def saveAsNewAPIHadoopDataset ( self , conf , keyConverter = None , valueConverter = None ) : jconf = self . ctx . _dictToJavaMap ( conf ) pickledRDD = self . _pickled ( ) self . ctx . _jvm . PythonRDD . saveAsHadoopDataset ( pickledRDD . _jrdd , True , jconf , keyConverter , valueConverter , True )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1414-L1440<CODESPLIT>RDD.saveAsNewAPIHadoopFile<CODESPLIT>Output a Python RDD of key - value pairs ( of form C { RDD [ ( K V ) ] } ) to any Hadoop file system using the new Hadoop OutputFormat API ( mapreduce package ) . Key and value types will be inferred if not specified . Keys and values are converted for output using either user specified converters or L { org . apache . spark . api . python . JavaToWritableConverter } . The C { conf } is applied on top of the base Hadoop conf associated with the SparkContext of this RDD to create a merged Hadoop MapReduce job configuration for saving the data .<CODESPLIT>def saveAsNewAPIHadoopFile ( self , path , outputFormatClass , keyClass = None , valueClass = None , keyConverter = None , valueConverter = None , conf = None ) : jconf = self . ctx . _dictToJavaMap ( conf ) pickledRDD = self . _pickled ( ) self . ctx . _jvm . PythonRDD . saveAsNewAPIHadoopFile ( pickledRDD . _jrdd , True , path , outputFormatClass , keyClass , valueClass , keyConverter , valueConverter , jconf )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1489-L1503<CODESPLIT>RDD.saveAsSequenceFile<CODESPLIT>Output a Python RDD of key - value pairs ( of form C { RDD [ ( K V ) ] } ) to any Hadoop file system using the L { org . apache . hadoop . io . Writable } types that we convert from the RDD s key and value types . The mechanism is as follows :<CODESPLIT>def saveAsSequenceFile ( self , path , compressionCodecClass = None ) : pickledRDD = self . _pickled ( ) self . ctx . _jvm . PythonRDD . saveAsSequenceFile ( pickledRDD . _jrdd , True , path , compressionCodecClass )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1505-L1521<CODESPLIT>RDD.saveAsPickleFile<CODESPLIT>Save this RDD as a SequenceFile of serialized objects . The serializer used is L { pyspark . serializers . PickleSerializer } default batch size is 10 .<CODESPLIT>def saveAsPickleFile ( self , path , batchSize = 10 ) : if batchSize == 0 : ser = AutoBatchedSerializer ( PickleSerializer ( ) ) else : ser = BatchedSerializer ( PickleSerializer ( ) , batchSize ) self . _reserialize ( ser ) . _jrdd . saveAsObjectFile ( path )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1524-L1572<CODESPLIT>RDD.saveAsTextFile<CODESPLIT>Save this RDD as a text file using string representations of elements .<CODESPLIT>def saveAsTextFile ( self , path , compressionCodecClass = None ) : def func ( split , iterator ) : for x in iterator : if not isinstance ( x , ( unicode , bytes ) ) : x = unicode ( x ) if isinstance ( x , unicode ) : x = x . encode ( "utf-8" ) yield x keyed = self . mapPartitionsWithIndex ( func ) keyed . _bypass_serializer = True if compressionCodecClass : compressionCodec = self . ctx . _jvm . java . lang . Class . forName ( compressionCodecClass ) keyed . _jrdd . map ( self . ctx . _jvm . BytesToString ( ) ) . saveAsTextFile ( path , compressionCodec ) else : keyed . _jrdd . map ( self . ctx . _jvm . BytesToString ( ) ) . saveAsTextFile ( path )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1611-L1627<CODESPLIT>RDD.reduceByKey<CODESPLIT>Merge the values for each key using an associative and commutative reduce function .<CODESPLIT>def reduceByKey ( self , func , numPartitions = None , partitionFunc = portable_hash ) : return self . combineByKey ( lambda x : x , func , func , numPartitions , partitionFunc )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1629-L1654<CODESPLIT>RDD.reduceByKeyLocally<CODESPLIT>Merge the values for each key using an associative and commutative reduce function but return the results immediately to the master as a dictionary .<CODESPLIT>def reduceByKeyLocally ( self , func ) : func = fail_on_stopiteration ( func ) def reducePartition ( iterator ) : m = { } for k , v in iterator : m [ k ] = func ( m [ k ] , v ) if k in m else v yield m def mergeMaps ( m1 , m2 ) : for k , v in m2 . items ( ) : m1 [ k ] = func ( m1 [ k ] , v ) if k in m1 else v return m1 return self . mapPartitions ( reducePartition ) . reduce ( mergeMaps )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1742-L1810<CODESPLIT>RDD.partitionBy<CODESPLIT>Return a copy of the RDD partitioned using the specified partitioner .<CODESPLIT>def partitionBy ( self , numPartitions , partitionFunc = portable_hash ) : if numPartitions is None : numPartitions = self . _defaultReducePartitions ( ) partitioner = Partitioner ( numPartitions , partitionFunc ) if self . partitioner == partitioner : return self # Transferring O(n) objects to Java is too expensive. # Instead, we'll form the hash buckets in Python, # transferring O(numPartitions) objects to Java. # Each object is a (splitNumber, [objects]) pair. # In order to avoid too huge objects, the objects are # grouped into chunks. outputSerializer = self . ctx . _unbatched_serializer limit = ( _parse_memory ( self . ctx . _conf . get ( "spark.python.worker.memory" , "512m" ) ) / 2 ) def add_shuffle_key ( split , iterator ) : buckets = defaultdict ( list ) c , batch = 0 , min ( 10 * numPartitions , 1000 ) for k , v in iterator : buckets [ partitionFunc ( k ) % numPartitions ] . append ( ( k , v ) ) c += 1 # check used memory and avg size of chunk of objects if ( c % 1000 == 0 and get_used_memory ( ) > limit or c > batch ) : n , size = len ( buckets ) , 0 for split in list ( buckets . keys ( ) ) : yield pack_long ( split ) d = outputSerializer . dumps ( buckets [ split ] ) del buckets [ split ] yield d size += len ( d ) avg = int ( size / n ) >> 20 # let 1M < avg < 10M if avg < 1 : batch *= 1.5 elif avg > 10 : batch = max ( int ( batch / 1.5 ) , 1 ) c = 0 for split , items in buckets . items ( ) : yield pack_long ( split ) yield outputSerializer . dumps ( items ) keyed = self . mapPartitionsWithIndex ( add_shuffle_key , preservesPartitioning = True ) keyed . _bypass_serializer = True with SCCallSiteSync ( self . context ) as css : pairRDD = self . ctx . _jvm . PairwiseRDD ( keyed . _jrdd . rdd ( ) ) . asJavaPairRDD ( ) jpartitioner = self . ctx . _jvm . PythonPartitioner ( numPartitions , id ( partitionFunc ) ) jrdd = self . ctx . _jvm . PythonRDD . valueOfPair ( pairRDD . partitionBy ( jpartitioner ) ) rdd = RDD ( jrdd , self . ctx , BatchedSerializer ( outputSerializer ) ) rdd . partitioner = partitioner return rdd
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1813-L1874<CODESPLIT>RDD.combineByKey<CODESPLIT>Generic function to combine the elements for each key using a custom set of aggregation functions .<CODESPLIT>def combineByKey ( self , createCombiner , mergeValue , mergeCombiners , numPartitions = None , partitionFunc = portable_hash ) : if numPartitions is None : numPartitions = self . _defaultReducePartitions ( ) serializer = self . ctx . serializer memory = self . _memory_limit ( ) agg = Aggregator ( createCombiner , mergeValue , mergeCombiners ) def combineLocally ( iterator ) : merger = ExternalMerger ( agg , memory * 0.9 , serializer ) merger . mergeValues ( iterator ) return merger . items ( ) locally_combined = self . mapPartitions ( combineLocally , preservesPartitioning = True ) shuffled = locally_combined . partitionBy ( numPartitions , partitionFunc ) def _mergeCombiners ( iterator ) : merger = ExternalMerger ( agg , memory , serializer ) merger . mergeCombiners ( iterator ) return merger . items ( ) return shuffled . mapPartitions ( _mergeCombiners , preservesPartitioning = True )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1876-L1891<CODESPLIT>RDD.aggregateByKey<CODESPLIT>Aggregate the values of each key using given combine functions and a neutral zero value . This function can return a different result type U than the type of the values in this RDD V . Thus we need one operation for merging a V into a U and one operation for merging two U s The former operation is used for merging values within a partition and the latter is used for merging values between partitions . To avoid memory allocation both of these functions are allowed to modify and return their first argument instead of creating a new U .<CODESPLIT>def aggregateByKey ( self , zeroValue , seqFunc , combFunc , numPartitions = None , partitionFunc = portable_hash ) : def createZero ( ) : return copy . deepcopy ( zeroValue ) return self . combineByKey ( lambda v : seqFunc ( createZero ( ) , v ) , seqFunc , combFunc , numPartitions , partitionFunc )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1893-L1909<CODESPLIT>RDD.foldByKey<CODESPLIT>Merge the values for each key using an associative function func and a neutral zeroValue which may be added to the result an arbitrary number of times and must not change the result ( e . g . 0 for addition or 1 for multiplication . ) .<CODESPLIT>def foldByKey ( self , zeroValue , func , numPartitions = None , partitionFunc = portable_hash ) : def createZero ( ) : return copy . deepcopy ( zeroValue ) return self . combineByKey ( lambda v : func ( createZero ( ) , v ) , func , func , numPartitions , partitionFunc )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1915-L1958<CODESPLIT>RDD.groupByKey<CODESPLIT>Group the values for each key in the RDD into a single sequence . Hash - partitions the resulting RDD with numPartitions partitions .<CODESPLIT>def groupByKey ( self , numPartitions = None , partitionFunc = portable_hash ) : def createCombiner ( x ) : return [ x ] def mergeValue ( xs , x ) : xs . append ( x ) return xs def mergeCombiners ( a , b ) : a . extend ( b ) return a memory = self . _memory_limit ( ) serializer = self . _jrdd_deserializer agg = Aggregator ( createCombiner , mergeValue , mergeCombiners ) def combine ( iterator ) : merger = ExternalMerger ( agg , memory * 0.9 , serializer ) merger . mergeValues ( iterator ) return merger . items ( ) locally_combined = self . mapPartitions ( combine , preservesPartitioning = True ) shuffled = locally_combined . partitionBy ( numPartitions , partitionFunc ) def groupByKey ( it ) : merger = ExternalGroupBy ( agg , memory , serializer ) merger . mergeCombiners ( it ) return merger . items ( ) return shuffled . mapPartitions ( groupByKey , True ) . mapValues ( ResultIterable )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1960-L1972<CODESPLIT>RDD.flatMapValues<CODESPLIT>Pass each value in the key - value pair RDD through a flatMap function without changing the keys ; this also retains the original RDD s partitioning .<CODESPLIT>def flatMapValues ( self , f ) : flat_map_fn = lambda kv : ( ( kv [ 0 ] , x ) for x in f ( kv [ 1 ] ) ) return self . flatMap ( flat_map_fn , preservesPartitioning = True )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1974-L1986<CODESPLIT>RDD.mapValues<CODESPLIT>Pass each value in the key - value pair RDD through a map function without changing the keys ; this also retains the original RDD s partitioning .<CODESPLIT>def mapValues ( self , f ) : map_values_fn = lambda kv : ( kv [ 0 ] , f ( kv [ 1 ] ) ) return self . map ( map_values_fn , preservesPartitioning = True )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2016-L2035<CODESPLIT>RDD.sampleByKey<CODESPLIT>Return a subset of this RDD sampled by key ( via stratified sampling ) . Create a sample of this RDD using variable sampling rates for different keys as specified by fractions a key to sampling rate map .<CODESPLIT>def sampleByKey ( self , withReplacement , fractions , seed = None ) : for fraction in fractions . values ( ) : assert fraction >= 0.0 , "Negative fraction value: %s" % fraction return self . mapPartitionsWithIndex ( RDDStratifiedSampler ( withReplacement , fractions , seed ) . func , True )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2037-L2050<CODESPLIT>RDD.subtractByKey<CODESPLIT>Return each ( key value ) pair in C { self } that has no pair with matching key in C { other } .<CODESPLIT>def subtractByKey ( self , other , numPartitions = None ) : def filter_func ( pair ) : key , ( val1 , val2 ) = pair return val1 and not val2 return self . cogroup ( other , numPartitions ) . filter ( filter_func ) . flatMapValues ( lambda x : x [ 0 ] )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2052-L2063<CODESPLIT>RDD.subtract<CODESPLIT>Return each value in C { self } that is not contained in C { other } .<CODESPLIT>def subtract ( self , other , numPartitions = None ) : # note: here 'True' is just a placeholder rdd = other . map ( lambda x : ( x , True ) ) return self . map ( lambda x : ( x , True ) ) . subtractByKey ( rdd , numPartitions ) . keys ( )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2095-L2115<CODESPLIT>RDD.coalesce<CODESPLIT>Return a new RDD that is reduced into numPartitions partitions .<CODESPLIT>def coalesce ( self , numPartitions , shuffle = False ) : if shuffle : # Decrease the batch size in order to distribute evenly the elements across output # partitions. Otherwise, repartition will possibly produce highly skewed partitions. batchSize = min ( 10 , self . ctx . _batchSize or 1024 ) ser = BatchedSerializer ( PickleSerializer ( ) , batchSize ) selfCopy = self . _reserialize ( ser ) jrdd_deserializer = selfCopy . _jrdd_deserializer jrdd = selfCopy . _jrdd . coalesce ( numPartitions , shuffle ) else : jrdd_deserializer = self . _jrdd_deserializer jrdd = self . _jrdd . coalesce ( numPartitions , shuffle ) return RDD ( jrdd , self . ctx , jrdd_deserializer )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2117-L2157<CODESPLIT>RDD.zip<CODESPLIT>Zips this RDD with another one returning key - value pairs with the first element in each RDD second element in each RDD etc . Assumes that the two RDDs have the same number of partitions and the same number of elements in each partition ( e . g . one was made through a map on the other ) .<CODESPLIT>def zip ( self , other ) : def get_batch_size ( ser ) : if isinstance ( ser , BatchedSerializer ) : return ser . batchSize return 1 # not batched def batch_as ( rdd , batchSize ) : return rdd . _reserialize ( BatchedSerializer ( PickleSerializer ( ) , batchSize ) ) my_batch = get_batch_size ( self . _jrdd_deserializer ) other_batch = get_batch_size ( other . _jrdd_deserializer ) if my_batch != other_batch or not my_batch : # use the smallest batchSize for both of them batchSize = min ( my_batch , other_batch ) if batchSize <= 0 : # auto batched or unlimited batchSize = 100 other = batch_as ( other , batchSize ) self = batch_as ( self , batchSize ) if self . getNumPartitions ( ) != other . getNumPartitions ( ) : raise ValueError ( "Can only zip with RDD which has the same number of partitions" ) # There will be an Exception in JVM if there are different number # of items in each partitions. pairRDD = self . _jrdd . zip ( other . _jrdd ) deserializer = PairDeserializer ( self . _jrdd_deserializer , other . _jrdd_deserializer ) return RDD ( pairRDD , self . ctx , deserializer )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2159-L2184<CODESPLIT>RDD.zipWithIndex<CODESPLIT>Zips this RDD with its element indices .<CODESPLIT>def zipWithIndex ( self ) : starts = [ 0 ] if self . getNumPartitions ( ) > 1 : nums = self . mapPartitions ( lambda it : [ sum ( 1 for i in it ) ] ) . collect ( ) for i in range ( len ( nums ) - 1 ) : starts . append ( starts [ - 1 ] + nums [ i ] ) def func ( k , it ) : for i , v in enumerate ( it , starts [ k ] ) : yield v , i return self . mapPartitionsWithIndex ( func )
1<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2186-L2204<CODESPLIT>RDD.zipWithUniqueId<CODESPLIT>Zips this RDD with generated unique Long ids .<CODESPLIT>def zipWithUniqueId ( self ) : n = self . getNumPartitions ( ) def func ( k , it ) : for i , v in enumerate ( it ) : yield v , i * n + k return self . mapPartitionsWithIndex ( func )